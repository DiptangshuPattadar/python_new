{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAuHIBc+jTEGUX52JpQ3Sb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiptangshuPattadar/python_new/blob/main/Assignment_9_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords\n"
      ],
      "metadata": {
        "id": "LQIl_8dd1CMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"\"\"Technology is evolving rapidly! It affects how we communicate, learn, and work.\n",
        "From smartphones to AI-powered assistants, every innovation makes life easier.\n",
        "However, privacy concerns are rising. Overall, technology shapes our modern world in profound ways.\"\"\"\n",
        "\n",
        "text_lower = text.lower()\n",
        "text_no_punct = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "words = tokenizer.tokenize(text_no_punct)\n",
        "\n",
        "\n",
        "sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nFallback Sentence Split:\\n\", sentences)\n",
        "print(\"\\nFiltered Words (No Stopwords):\\n\", filtered_words)\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWnPwRMR1KhU",
        "outputId": "f6200f2e-25aa-430b-c69f-f4fb1db818b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Technology is evolving rapidly! It affects how we communicate, learn, and work. \n",
            "From smartphones to AI-powered assistants, every innovation makes life easier. \n",
            "However, privacy concerns are rising. Overall, technology shapes our modern world in profound ways.\n",
            "\n",
            "Fallback Sentence Split:\n",
            " ['Technology is evolving rapidly! It affects how we communicate, learn, and work', 'From smartphones to AI-powered assistants, every innovation makes life easier', 'However, privacy concerns are rising', 'Overall, technology shapes our modern world in profound ways']\n",
            "\n",
            "Filtered Words (No Stopwords):\n",
            " ['technology', 'evolving', 'rapidly', 'affects', 'communicate', 'learn', 'work', 'smartphones', 'aipowered', 'assistants', 'every', 'innovation', 'makes', 'life', 'easier', 'however', 'privacy', 'concerns', 'rising', 'overall', 'technology', 'shapes', 'modern', 'world', 'profound', 'ways']\n",
            "\n",
            "Word Frequency Distribution:\n",
            "technology: 2\n",
            "evolving: 1\n",
            "rapidly: 1\n",
            "affects: 1\n",
            "communicate: 1\n",
            "learn: 1\n",
            "work: 1\n",
            "smartphones: 1\n",
            "aipowered: 1\n",
            "assistants: 1\n",
            "every: 1\n",
            "innovation: 1\n",
            "makes: 1\n",
            "life: 1\n",
            "easier: 1\n",
            "however: 1\n",
            "privacy: 1\n",
            "concerns: 1\n",
            "rising: 1\n",
            "overall: 1\n",
            "shapes: 1\n",
            "modern: 1\n",
            "world: 1\n",
            "profound: 1\n",
            "ways: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and LemmaƟzaƟon\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques.\n"
      ],
      "metadata": {
        "id": "eNZFA_Zf4oa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Word\\tPorter\\tLancaster\\tLemma\")\n",
        "for word in filtered_words:\n",
        "    print(f\"{word}\\t{porter.stem(word)}\\t{lancaster.stem(word)}\\t{lemmatizer.lemmatize(word)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoLjIgcn19Qx",
        "outputId": "81cc4236-a168-40b9-d518-9e455a15df44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word\tPorter\tLancaster\tLemma\n",
            "technology\ttechnolog\ttechnolog\ttechnology\n",
            "evolving\tevolv\tevolv\tevolving\n",
            "rapidly\trapidli\trapid\trapidly\n",
            "affects\taffect\taffect\taffect\n",
            "communicate\tcommun\tcommun\tcommunicate\n",
            "learn\tlearn\tlearn\tlearn\n",
            "work\twork\twork\twork\n",
            "smartphones\tsmartphon\tsmartphon\tsmartphones\n",
            "aipowered\taipow\taipow\taipowered\n",
            "assistants\tassist\tassist\tassistant\n",
            "every\teveri\tevery\tevery\n",
            "innovation\tinnov\tinnov\tinnovation\n",
            "makes\tmake\tmak\tmake\n",
            "life\tlife\tlif\tlife\n",
            "easier\teasier\teasy\teasier\n",
            "however\thowev\thowev\thowever\n",
            "privacy\tprivaci\tpriv\tprivacy\n",
            "concerns\tconcern\tconcern\tconcern\n",
            "rising\trise\tris\trising\n",
            "overall\toveral\toveral\toverall\n",
            "technology\ttechnolog\ttechnolog\ttechnology\n",
            "shapes\tshape\tshap\tshape\n",
            "modern\tmodern\tmodern\tmodern\n",
            "world\tworld\tworld\tworld\n",
            "profound\tprofound\tprofound\tprofound\n",
            "ways\tway\tway\tway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Spliƫng\n",
        "1. Take their original text from QuesƟon 1.\n",
        "2. Use regular expressions to:\n",
        "a. Extract all words with more than 5 leƩers.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text spliƫng techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "b. Extract words starƟng with a vowel."
      ],
      "metadata": {
        "id": "hc0CW_DX4tfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "more_than_5_letters = re.findall(r'\\b\\w{6,}\\b', text)\n",
        "numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "\n",
        "# Words starting with a vowel\n",
        "vowel_words = [word for word in alpha_words if word[0].lower() in 'aeiou']\n",
        "\n",
        "print(\"More than 5 letters:\", more_than_5_letters)\n",
        "print(\"Numbers:\", numbers)\n",
        "print(\"Capitalized Words:\", capitalized)\n",
        "print(\"Alphabet-only Words:\", alpha_words)\n",
        "print(\"Words starting with vowels:\", vowel_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm6bMebX4PYm",
        "outputId": "789e1786-20dc-4f23-e0d6-5f5bb3ee4a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "More than 5 letters: ['Technology', 'evolving', 'rapidly', 'affects', 'communicate', 'smartphones', 'powered', 'assistants', 'innovation', 'easier', 'However', 'privacy', 'concerns', 'rising', 'Overall', 'technology', 'shapes', 'modern', 'profound']\n",
            "Numbers: []\n",
            "Capitalized Words: ['Technology', 'It', 'From', 'However', 'Overall']\n",
            "Alphabet-only Words: ['Technology', 'is', 'evolving', 'rapidly', 'It', 'affects', 'how', 'we', 'communicate', 'learn', 'and', 'work', 'From', 'smartphones', 'to', 'AI', 'powered', 'assistants', 'every', 'innovation', 'makes', 'life', 'easier', 'However', 'privacy', 'concerns', 'are', 'rising', 'Overall', 'technology', 'shapes', 'our', 'modern', 'world', 'in', 'profound', 'ways']\n",
            "Words starting with vowels: ['is', 'evolving', 'It', 'affects', 'and', 'AI', 'assistants', 'every', 'innovation', 'easier', 'are', 'Overall', 'our', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "1. Take original text from QuesƟon 1.\n",
        "2. Write a custom tokenizaƟon funcƟon that:\n",
        "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is)."
      ],
      "metadata": {
        "id": "F4GTmYy34wiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_tokenizer(text):\n",
        "\n",
        "    tokens = re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|\\d+\\.\\d+|\\d+\", text)\n",
        "    return tokens\n",
        "\n",
        "custom_tokens = custom_tokenizer(text)\n",
        "print(\"Custom Tokens:\", custom_tokens)\n",
        "\n",
        "\n",
        "sample_text = \"Contact me at john.doe@example.com or visit https://example.com. Call me at +91 9876543210.\"\n",
        "\n",
        "\n",
        "sample_text = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b', '<EMAIL>', sample_text)\n",
        "\n",
        "sample_text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', sample_text)\n",
        "\n",
        "sample_text = re.sub(r'(\\+91\\s\\d{10}|\\d{3}-\\d{3}-\\d{4})', '<PHONE>', sample_text)\n",
        "\n",
        "print(\"Cleaned Text:\", sample_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dqg5TVV4fQK",
        "outputId": "a715e9ad-3b7a-446f-987c-6394da31d967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokens: ['Technology', 'is', 'evolving', 'rapidly', 'It', 'affects', 'how', 'we', 'communicate', 'learn', 'and', 'work', 'From', 'smartphones', 'to', 'AI-powered', 'assistants', 'every', 'innovation', 'makes', 'life', 'easier', 'However', 'privacy', 'concerns', 'are', 'rising', 'Overall', 'technology', 'shapes', 'our', 'modern', 'world', 'in', 'profound', 'ways']\n",
            "Cleaned Text: Contact me at <EMAIL> or visit <URL> Call me at <PHONE>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "630b3f5F4kCZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}